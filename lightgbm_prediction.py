# -*- coding: utf-8 -*-
"""LightGBM prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KjJZFp-GGKdL5LVZs6blzFqBVmH1vjAu
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
pd.set_option('max_rows', 300)
pd.set_option('max_columns', 300)
import os
import glob
from google.colab import drive 
drive.mount('/content/gdrive')
import warnings
warnings.filterwarnings('ignore')
data_dir = '../content/gdrive/MyDrive/optiver-realized-volatility-prediction/'
stock_id = 0
df = pd.DataFrame({'A': [1, 1, 1, 2, 3, 4], 'B': [8, 2, 3, 4, 5, 6]})
df['C'] = df['B'].apply(lambda x:f'{stock_id} - {x}')
print(f'{stock_id}-{1}')
print(df)
df_1 = df.groupby('A')['B'].apply(np.max)
print(df_1)

"""**Functions for preprocess**

"""

def calc_wap(df):
    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1'])/(df['bid_size1'] + df['ask_size1'])
    return wap
def calc_wap2(df):
    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2'])/(df['bid_size2'] + df['ask_size2'])
    return wap
def log_return(list_stock_prices):
    return np.log(list_stock_prices).diff() 
def realized_volatility(series):
    return np.sqrt(np.sum(series**2))
def count_unique(series):
    return len(np.unique(series))
book_train = pd.read_parquet(data_dir + "book_train.parquet/stock_id=1")
book_train.head()

"""**Main function of preprocessing book data**"""

# Commented out IPython magic to ensure Python compatibility.
def preprocessor_book(file_path):
    df = pd.read_parquet(file_path)
    #calculate return etc
    df['wap'] = calc_wap(df)
    df['log_return'] = df.groupby('time_id')['wap'].apply(log_return)
    
    df['wap2'] = calc_wap2(df)
    df['log_return2'] = df.groupby('time_id')['wap2'].apply(log_return)
    
    df['wap_balance'] = abs(df['wap'] - df['wap2'])
    
    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1'])/2)
    df['bid_spread'] = df['bid_price1'] - df['bid_price2']
    df['ask_spread'] = df['ask_price1'] - df['ask_price2']
    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])
    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))
     #dict for aggregate
    # create_feature_dict = {
    #     'log_return':[realized_volatility],
    #     'log_return2':[realized_volatility],
    #     'wap_balance':[np.mean],
    #     'price_spread':[np.mean],
    #     'bid_spread':[np.mean],
    #     'ask_spread':[np.mean],
    #     'volume_imbalance':[np.mean],
    #     'total_volume':[np.mean],
    #     'wap':[np.mean],
    #         }
    create_feature_dict = {
        'wap': [np.sum, np.mean, np.std],
        'wap2': [np.sum, np.mean, np.std],
        'log_return': [np.sum, realized_volatility, np.mean, np.std],
        'log_return2': [np.sum, realized_volatility, np.mean, np.std],
        'wap_balance': [np.sum, np.mean, np.std],
        'price_spread':[np.sum, np.mean, np.std],
        'bid_spread':[np.sum, np.mean, np.std],
        'ask_spread':[np.sum, np.mean, np.std],
        'total_volume':[np.sum, np.mean, np.std],
        'volume_imbalance':[np.sum, np.mean, np.std]
    }

    #####groupby / all seconds
    df_feature = pd.DataFrame(df.groupby(['time_id']).agg(create_feature_dict)).reset_index()
    df_feature.columns = ['_'.join(col) for col in df_feature.columns] #time_id is changed to time_id_
        
    ######groupby / last XX seconds
    last_seconds = [100,300,500]
    
    for second in last_seconds:
        second = 600 - second 
    
        df_feature_sec = pd.DataFrame(df.query(f'seconds_in_bucket >= {second}').groupby(['time_id']).agg(create_feature_dict)).reset_index()
        df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns] #time_id is changed to time_id_
     
        df_feature_sec = df_feature_sec.add_suffix('_' + str(second))

        df_feature = pd.merge(df_feature,df_feature_sec,how='left',left_on='time_id_',right_on=f'time_id__{second}')
        df_feature = df_feature.drop([f'time_id__{second}'],axis=1)
    
    #create row_id
    stock_id = file_path.split('=')[1]
    print(file_path.split('='))
    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x:f'{stock_id}-{x}')
    df_feature = df_feature.drop(['time_id_'],axis=1)
    
    return df_feature
# %time
file_path = data_dir + "book_train.parquet/stock_id=0"
preprocessor_book(file_path)

"""**Main function of preprocessing trade data**"""

# Commented out IPython magic to ensure Python compatibility.
def preprocessor_trade(file_path):
    df = pd.read_parquet(file_path)
    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)
    
    #can change this function such as 'size':[np.sum,np.mean,np.std]
    aggregate_dictionary = {
        'log_return':[realized_volatility],
        'seconds_in_bucket':[count_unique],
        'size':[np.sum],
        'order_count':[np.mean],
    }
    
    df_feature = df.groupby('time_id').agg(aggregate_dictionary).reset_index()
    #df_feature = df_feature.reset_index()
    df_feature.columns = ['_'.join(col) for col in df_feature.columns]

    
    ######groupby / last XX seconds
    last_seconds = [100,300,500]
    
    for second in last_seconds:
        second = 600 - second #can change second
    
        df_feature_sec = df.query(f'seconds_in_bucket >= {second}').groupby('time_id').agg(aggregate_dictionary)
        df_feature_sec = df_feature_sec.reset_index()
        
        df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns]
        df_feature_sec = df_feature_sec.add_suffix('_' + str(second))
        df_feature = pd.merge(df_feature,df_feature_sec,how='left',left_on='time_id_',right_on=f'time_id__{second}')
        df_feature = df_feature.drop([f'time_id__{second}'],axis=1)
    
    df_feature = df_feature.add_prefix('trade_')
    stock_id = file_path.split('=')[1]
    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')
    df_feature = df_feature.drop(['trade_time_id_'],axis=1) 
    return df_feature
# %time
file_path = data_dir + "trade_train.parquet/stock_id=0"
preprocessor_trade(file_path)

"""**Function to get group stats of time_id and stock_id**"""

def get_time_stock(df):
  df['stock_id'] = df['row_id'].apply(lambda x:x.split('-')[0])
  df['time_id'] = df['row_id'].apply(lambda x:x.split('-')[1])
  #get volatility columns
  vol_cols = ['log_return_realized_volatility', 'log_return2_realized_volatility', 'log_return_realized_volatility_100',
               'log_return2_realized_volatility_100', 
                'log_return_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return_realized_volatility_500', 'log_return2_realized_volatility_500', 
                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_500', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_100']
  #gr oup by the stock id
  df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean','std','max','min']).reset_index()
  #rename columns joining suffix
  df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]
  df_stock_id = df_stock_id.add_suffix('_'+'stock')
  #group by the time id
  df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean','std','max','min']).reset_index()
  #rename columns joining suffix
  df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]
  df_time_id = df_time_id.add_suffix('_'+'time')
  # Merge with original dataframe
  df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])
  df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])
  df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)
  df.drop(['stock_id', 'time_id'], axis = 1, inplace = True)
  return df

"""**Combined preprocessor fucntion**"""

def preprocessor(list_stock_ids, is_train = True):
    from joblib import Parallel, delayed # parallel computing to save time
    #df = pd.DataFrame()
    
    def for_joblib(stock_id):
        if is_train:
            file_path_book = data_dir + "book_train.parquet/stock_id=" + str(stock_id)
            file_path_trade = data_dir + "trade_train.parquet/stock_id=" + str(stock_id)
        else:
            file_path_book = data_dir + "book_test.parquet/stock_id=" + str(stock_id)
            file_path_trade = data_dir + "trade_test.parquet/stock_id=" + str(stock_id)
            
        df_tmp = pd.merge(preprocessor_book(file_path_book),preprocessor_trade(file_path_trade),on='row_id',how='left')
        #return pd.concat([df,df_tmp])
        return df_tmp
    df = Parallel(n_jobs=-1, verbose=1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)
    df = pd.concat(df,ignore_index = True)
    return df
list_stock_ids = [0,1]
preprocessor(list_stock_ids, is_train = True)

"""**Traning set**"""

# Commented out IPython magic to ensure Python compatibility.
train = pd.read_csv(data_dir + 'train.csv')
train_ids = train.stock_id.unique()
# %time
df_train = preprocessor(list_stock_ids= train_ids, is_train = True)

train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)
train = train[['row_id','target']]
df_train = train.merge(df_train, on = ['row_id'], how = 'left')
df_train.head()

"""**Test set**"""

# Commented out IPython magic to ensure Python compatibility.
test = pd.read_csv(data_dir + 'test.csv')
test_ids = test.stock_id.unique()
# %time
df_test = preprocessor(list_stock_ids= test_ids, is_train = False)
df_test = test.merge(df_test, on = ['row_id'], how = 'left')

"""**Create stats by stock id and time id**"""

df_train = get_time_stock(df_train)
df_test = get_time_stock(df_test)

"""**Target encoding by stock_id**"""

from sklearn.model_selection import KFold
#stock_id target encoding
df_train['stock_id'] = df_train['row_id'].apply(lambda x:x.split('-')[0])
df_test['stock_id'] = df_test['row_id'].apply(lambda x:x.split('-')[0])

stock_id_target_mean = df_train.groupby('stock_id')['target'].mean() 
df_test['stock_id_target_enc'] = df_test['stock_id'].map(stock_id_target_mean) # test_set

#training
tmp = np.repeat(np.nan, df_train.shape[0])
kf = KFold(n_splits = 10, shuffle=True,random_state = 19911109)
for idx_1, idx_2 in kf.split(df_train):
    target_mean = df_train.iloc[idx_1].groupby('stock_id')['target'].mean()

    tmp[idx_2] = df_train['stock_id'].iloc[idx_2].map(target_mean)
df_train['stock_id_target_enc'] = tmp

"""**Model building**"""

print(df_train)
df_test.head()
DO_FEAT_IMP = False
if len(df_test)==3:
    DO_FEAT_IMP = True

"""**LightGBM**"""

import lightgbm as lgbm
def calc_model_importance(model, feature_names=None, importance_type='gain'):
    importance_df = pd.DataFrame(model.feature_importance(importance_type=importance_type),
                                 index=feature_names,
                                 columns=['importance']).sort_values('importance')
    return importance_df


def plot_importance(importance_df, title='',
                    save_filepath=None, figsize=(8, 12)):
    fig, ax = plt.subplots(figsize=figsize)
    importance_df.plot.barh(ax=ax)
    if title:
        plt.title(title)
    plt.tight_layout()
    if save_filepath is None:
        plt.show()
    else:
        plt.savefig(save_filepath)
    plt.close()

df_train['stock_id'] = df_train['stock_id'].astype(int)
df_test['stock_id'] = df_test['stock_id'].astype(int)
X = df_train.drop(['row_id','target'],axis=1)
y = df_train['target']
def rmspe(y_true, y_pred):
    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))

def feval_RMSPE(preds, lgbm_train):
    labels = lgbm_train.get_label()
    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False
params = {
      "objective": "rmse", 
      "metric": "rmse", 
      "boosting_type": "gbdt",
      'early_stopping_rounds': 30,
      'learning_rate': 0.01,
      'lambda_l1': 1,
      'lambda_l2': 1,
      'feature_fraction': 0.8,
      'bagging_fraction': 0.8,
  }

"""**Cross validation**"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.model_selection import KFold
kf = KFold(n_splits=5, random_state=19901028, shuffle=True)
oof = pd.DataFrame()                 # out-of-fold result
models = []                          # models
scores = 0.0                         # validation score

gain_importance_list = []
split_importance_list = []

# %time
for fold, (trn_idx, val_idx) in enumerate(kf.split(X, y)):

    print("Fold :", fold+1)
    
    # create dataset
    X_train, y_train = X.loc[trn_idx], y[trn_idx]
    X_valid, y_valid = X.loc[val_idx], y[val_idx]
    
    #RMSPE weight
    weights = 1/np.square(y_train)
    lgbm_train = lgbm.Dataset(X_train,y_train,weight = weights)

    weights = 1/np.square(y_valid)
    lgbm_valid = lgbm.Dataset(X_valid,y_valid,reference = lgbm_train,weight = weights)
    # model 
    model = lgbm.train(params=params,
                      train_set=lgbm_train,
                      valid_sets=[lgbm_train, lgbm_valid],
                      num_boost_round=5000,         
                      feval=feval_RMSPE,
                      verbose_eval=100,
                      categorical_feature = ['stock_id']                
                     )
    
    # validation 
    y_pred = model.predict(X_valid, num_iteration=model.best_iteration)

    RMSPE = round(rmspe(y_true = y_valid, y_pred = y_pred),3)
    print(f'Performance of theã€€prediction: , RMSPE: {RMSPE}')

    #keep scores and models
    scores += RMSPE / 5
    models.append(model)
    print("*" * 100)
    # --- calc model feature importance ---
    if DO_FEAT_IMP:    
        feature_names = X_train.columns.values.tolist()
        gain_importance_df = calc_model_importance(
            model, feature_names=feature_names, importance_type='gain')
        gain_importance_list.append(gain_importance_df)

        split_importance_df = calc_model_importance(
            model, feature_names=feature_names, importance_type='split')
        split_importance_list.append(split_importance_df)

print('scores', scores)
def calc_mean_importance(importance_df_list):
    mean_importance = np.mean(
        np.array([df['importance'].values for df in importance_df_list]), axis=0)
    mean_df = importance_df_list[0].copy()
    mean_df['importance'] = mean_importance
    return mean_df
if DO_FEAT_IMP:
    mean_gain_df = calc_mean_importance(gain_importance_list)
    plot_importance(mean_gain_df, title='Model feature importance by gain')
    mean_gain_df = mean_gain_df.reset_index().rename(columns={'index': 'feature_names'})
    mean_gain_df.to_csv('gain_importance_mean.csv', index=False)
if DO_FEAT_IMP:
    mean_split_df = calc_mean_importance(split_importance_list)
    plot_importance(mean_split_df, title='Model feature importance by split')
    mean_split_df = mean_split_df.reset_index().rename(columns={'index': 'feature_names'})
    mean_split_df.to_csv('split_importance_mean.csv', index=False)

"""**Test set**"""

df_test.columns
print(df_test)
#df_train.columns
y_pred = df_test[['row_id']]
#X_test = df_test.drop(['time_id', 'row_id'], axis = 1)
X_test = df_test.drop(['row_id'], axis = 1)
print(X_test)
target = np.zeros(len(X_test))

#light gbm models
for model in models:
    pred = model.predict(X_test[X_valid.columns], num_iteration=model.best_iteration)
    target += pred / len(models)
    print('target',target)
print(target)
y_pred = y_pred.assign(target = target)
y_pred.to_csv('submission.csv',index = False)

>>> import numpy as np
>>> from sklearn.model_selection import KFold

>>> X = ["a", "b", "c", "d"]
>>> kf = KFold(n_splits=2)
for train, test in kf.split(X):
  print("%s %s" % (train, test))
  X = np.array([[0., 0.], [1., 1.], [-1., -1.], [2., 2.]])
  print(X)
  print('len',len(X))
  y = np.array([0, 1, 0, 1])
  X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]
  print(X_train)
  print(X_test)

# scikit-learn k-fold cross-validation
from numpy import array
from sklearn.model_selection import KFold
# data sample
data = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6])
# prepare cross validation
kfold = KFold(3)
# enumerate splits
for train, test in kfold.split(data):
	print('train: %s, test: %s' % (data[train], data[test]))
	print(train)
	print(test)

import pandas as pd
import numpy as np
df = pd.DataFrame(np.arange(12).reshape(3, 4),
                  columns=['A', 'B', 'C', 'D'])


df.drop(['B', 'C'], axis=1)